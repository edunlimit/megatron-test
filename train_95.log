XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 
Nodelist:=  train-st-p4d24xlarge-[1-2]
Number of nodes:=  2
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 
MASTER_PORT=29500
WORLD_SIZE=16
MASTER_ADDR=10.0.31.196
NODE_RANK=0
python /fsx/dataset/megatron-test/pretrain_qwen.py --save /home/ec2-user/model//checkpoint/pretrain-mcore-qwen2.5-7B-lr-0.0005-minlr-1e-6-bs-1-gbs-64-seqlen-4096-pr-bf16-tp-2-pp-1-cp-1-ac-sel-do-true-sp-true-ti-0-wi-0 --lr 0.0005 --min-lr 1e-6 --lr-decay-style cosine --weight-decay 0.1 --adam-beta1 0.9 --adam-beta2 0.95 --clip-grad 1.0 --init-method-std 0.008 --attention-dropout 0.0 --hidden-dropout 0.0 --lr-decay-iters 0 --lr-warmup-iters 0 --train-iters 0 --micro-batch-size 1 --global-batch-size 64 --num-layers 28 --hidden-size 3584 --num-attention-heads 28 --ffn-hidden-size 18944 --seq-length 4096 --max-position-embeddings 131072 --max-padding-length 4096 --log-interval 1 --log-throughput --eval-interval 10000 --eval-iters 10 --save-interval 2000 --tensorboard-queue-size 1 --tensorboard-dir /home/ec2-user/model//tensorboard/pretrain-mcore-qwen2.5-7B-lr-0.0005-minlr-1e-6-bs-1-gbs-64-seqlen-4096-pr-bf16-tp-2-pp-1-cp-1-ac-sel-do-true-sp-true-ti-0-wi-0_2025.03.19-08.38.49 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensor-model-parallel-size 2 --pipeline-model-parallel-size 1 --context-parallel-size 1 --no-load-optim --no-load-rng --num-workers 8 --extra-vocab-size 421 --patch-tokenizer-type Qwen2Tokenizer --swiglu --normalization RMSNorm --norm-epsilon 1e-6 --use-rotary-position-embeddings --position-embedding-type rope --disable-bias-linear --add-qkv-bias --rotary-percent 1.0 --rotary-base 1000000 --rotary-seq-len-interpolation-factor 1 --no-save-optim --data-path /fsx/dataset/qwen_codeparrot_content_document --split 99,1,0 --dataset MMAP --bf16 --load /fsx/dataset/megatron-qwen2.5-7B/release/mp_rank_00/ --transformer-impl transformer_engine --recompute-activations --use-distributed-optimizer --sequence-parallel --group-query-attention --num-query-groups 4 --tp-comm-overlap --overlap-grad-reduce --overlap-param-gather --train-mode pretrain --untie-embeddings-and-output-weights
/fsx/dataset/megatron-test/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/fsx/dataset/megatron-test/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/fsx/dataset/megatron-test/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/fsx/dataset/megatron-test/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/fsx/dataset/megatron-test/megatron/training/utils.py:22: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/fsx/dataset/megatron-test/megatron/core/optimizer/clip_grads.py:30: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
Traceback (most recent call last):
  File "/fsx/dataset/megatron-test/pretrain_qwen.py", line 87, in <module>
    from megatron_patch.model.qwen2.layer_specs import (
  File "/fsx/dataset/megatron-test/megatron_patch/model/qwen2/layer_specs.py", line 41, in <module>
    from .transformer.attention import SelfAttention, SelfAttentionSubmodules
  File "/fsx/dataset/megatron-test/megatron_patch/model/qwen2/transformer/attention.py", line 29, in <module>
    from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim
  File "/fsx/dataset/megatron-test/megatron/core/transformer/custom_layers/transformer_engine.py", line 9, in <module>
    import transformer_engine as te
ModuleNotFoundError: No module named 'transformer_engine'
